# About Project "Data Modeling with Postgres"

## The purpose and context

A startup company, Sparkify, want to do analyze data from user activities. Datas in JSON structure has been collected from Sparkify's new music stream app. 
A Postgres star scheme database designed to optimize queries on song play analysis. The database go with ETL pipeline for analytic purpose

## The project technical requirements
Apply data modeling with Postgres and build an ETL pipeline using Python.
For Postgres database, need to define fact and dimension tables for star schema that will be used for particlar analytic with SQL.
For Python ETL pipeline, this is the way transfer data from two directories of JSON files into Postgres database.

## Project Technical Guides
These guides included database schema model, project files, run Python script, JSON song and log file locations and some of example quieries for song play analytic.

### Database schema
![alt text](https://github.com/doanvanthanhfpt/DataModeling_with_postgresql/blob/main/sparkify_erd.png)

### Project files
Project includes 6 files

***test.ipynb*** displays the first few rows of each table to let you check your database.

***create_tables.py*** drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts.

***etl.ipynb*** reads and processes a single file from song_data and log_data and loads the data into your tables. This notebook contains detailed instructions on the ETL process for each of the tables.

***etl.py*** reads and processes files from song_data and log_data and loads them into your tables. You can fill this out based on your work in the ETL notebook.

***sql_queries.py*** contains all your sql queries, and is imported into the last three files above.

***README.md*** this file, provides discussion on your project.

### Project steps
Importance note: 
> Always run ***create_tables.py*** before run ***test.ipynb***, ***etl.ipynb***, ***etl.py***.

> Python script ***sql_queries.py*** includes necessary CREATE, DROP statements that called by ***create_tables.py***.

#### Run Python script
Lauch Terminal and then run *python create_tables.py*

#### Build ETL processes
Lauch notebook Jupyter to run ***etl.ipynb***.

Use notebook ***test.ipynb*** to test the results

#### Build ETL Pipeline
Connect processes from ***etl.ipynb*** into ***etl.py*** and then run ***etl.py*** from terminal
Note: > Rerun ***create_tables.py*** to reset your tables before each time you run this notebook.

### Datasets

#### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are file paths to two files in this dataset.
> song_data/A/B/C/TRABCEI128F424C983.json

> song_data/A/A/B/TRAABJL12903CDCF1A.json

#### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
> log_data/2018/11/2018-11-12-events.json

> log_data/2018/11/2018-11-13-events.json

